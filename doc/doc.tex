\documentclass[a4paper,oneside,12pt]{article}
\usepackage{polski}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{textgreek}

\usepackage[printwatermark]{xwatermark}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage[ampersand]{easylist}

\newwatermark[allpages,color=gray!30,angle=45,scale=3,xpos=0,ypos=0]{SZKIC}

\title{Projekt WEDT, wersja robocza}
\author{
	Michał Krawczak \\
	Adrian Więch \\
	Jakub Zawiślak
}
\date{\today}

\begin{document}

\maketitle
\newpage
\tableofcontents
\newpage

\setlength\parindent{0pt}
\setlength{\parskip}{0.25em}

\section{Temat i zakres projektu}

W ramach projektu badaliśmy możliwość wykorzystania technik uczenia maszynowego do ekstrakcji danych osób i instytucji z faktur.

\section{Opis zbioru testowego}

Zbiór testowy stanowią pliki tekstowe będące rezultatem przetworzenia zeskanowanych faktur przez program typu OCR.

\section{Badane algorytmy}

W ramach projektu zbadaliśmy dwa algorytmy:

\begin{easylist}[itemize]
& Prosty algorytm oparty na N-gramach
& Algorytm oparty na technice losowych lasów decyzyjnych (RDF)
\end{easylist}

Ponadto stworzyliśmy program o nazwie Tokenizer, służący do dzielenia faktur na tokeny i oznaczania charakterystycznych dla faktur typów tokenów. Jest on używany do wstępnego przetworzenia wejścia dla obu algorytmów.

\section{Opis programu Tokenizer}

\subsection{Podział na tokeny}

Pierwszym etapem przetwarzania faktury jest podział na tokeny. Tokeny powinny być, w miarę możliwości, odrębnymi jednostkami semantycznymi. 

Najprostszym rozwiązaniem mógłby być użycie spacji jako separatorów. Takie podejście ma jednak wady:

\begin{easylist}[itemize]
& W plikach tekstowych wygenerowanych przez OCR często brakuje spacji - szczególnie bezpośrednio przed i po znakach interpunkcyjnych, takich jak kropka, przecinek, nawias czy dwukropek. 
& Często chcielibyśmy traktować jako jeden token coś, co zawiera w sobie spacje. Przykładami mogą być wielowyrazowe nazwy miejscowości (np. Mińsk Mazowiecki) czy też numery kont bankowych podzielone na 4-cyfrowe bloki odseparowane spacjami (np. 12 3456 7890 0000 0000 0000 0001). Nie wszystkie spacje powinny być zatem uwzględniane jako separatory.
\end{easylist}

W przypadku pierwszym rozwiązaniem jest używanie jako separatorów nie tylko spacji, lecz także: dwukropków, kropek, nawiasów i przecinków. To jednak powoduje kolejny problem: podobnie jak spacje, te znaki w określonych sytuacjach nie powinny być separatorami. Na przykład, jeśli mamy dwa ciągi cyfr oddzielone kropką (123.45), to prawdopodobnie mamy do czynienia z liczbą wrac z rozwinięciem dziesiętnym. Podobnie, trzy grupy po dwie cyfry (01.01.16) prawdopodobnie stanowią datę.

Powyższe obserwacje doprowadziły do następującego algorytmu:
\begin{easylist}[itemize]
& Dla każdego znaku w dokumencie należącego do zbioru: {spacja, przecinek, kropka, nawias, średnik, nawias zamykający, nawias otwierający}:
&& Na podstawie otoczenia znaku stwierdź, czy wchodzi on w skład jednego z następujących tokenów:
&&& Nazwa miejscowości
&&& Nazwa ulicy
&&& Inne zesłownikowane wartości, jak np. "S. A." czy "Sp. z o.o."
&&& Numer konta
&&& Data
&&& Liczba dziesiętna
&&& NIP
&& Jeśli nie, to podziel tekst w tym miejscu
& Z wyniku powyższej operacji usuń puste tokeny
\end{easylist}

\subsection{Oznaczanie tokenów}

Każdy z tokenów jest przepuszczany przez szereg testów, ustawionych według priorytetu, z których każdemu testowi odpowiada inny typ tokenu. Jeśli którykolwiek test się powiedzie, token zostaje przyporządkowany do odpowiedniego typu. Testy są podzielone na dwa rodzaje:
\begin{easylist}[itemize]
& Słowniki
& Intuicje
\end{easylist} 
 
Test słownikowy polega na przejrzeniu danego słownika i sprawdzeniu, czy dany token się w nim znajduje. Do projektu użyliśmy następujących publicznie dostępnych słowników:
\begin{easylist}[itemize]
& Słownik nazw ulic
& Słownik nazw miejscowości
& Słownik nazw jednostek samorządu terytorialnego (gminy, powiaty etc.)
& Słownik imion
& Słownik nazwisk
\end{easylist}

\section{Podejście oparte na N-gramach}


\section{Podejście oparte na losowym lesie decyzyjnym}
Algorytm jest dwuetapowy.
Najpierw następuje etap uczenia algorytmu. Polega on na zbudowaniu wielu drzew decyzyjnych na podstawie danych treningowych. Poniżej znajdują się kolejne kroki wykonywane w tym procesie.
-	Załadowanie treningowych bloków adresowych do pamięci. Oznaczmy tę liczbę przez X.
-	Podjęcie decyzji ile drzew decyzyjnych zostanie wygenerowanych. Jest to liczbą stanowiąca 70\% liczby wszystkich załadowanych bloków adresowych (czyli 0,7*X).
-	Następnie w pętli generowane są kolejne drzewa decyzyjne. Dalsze kroki opisują proces generowania pojedynczego drzewa decyzyjnego.
-	Losowane są adresy jakie będą używane podczas tworzenia danego drzewa.  Losowanych jest X dresów ze zwracaniem.
-	Losowany jest zbiór istotnych tokenów. Jest to 80\% wszystkich tokenów występujących pośród załadowanych bloków adresowych.
-	Teraz następuje iteracyjne tworzenie węzłów drzewa na podstawie danych wylosowanych w punktach 4 i 5. W każdym węźle wykonywane są następujące kroki:
--	Losowana jest pewna liczba z przedziału <1, max długość bloku adresowego w węźle>. Oznaczamy ją przez Z,
--	Losowany jest zbiór tokenów, którego liczność należy do przedziału <3, 3+40\%*liczba z punktu wyżej). Oznaczamy go przez Y,
--	A następnie stawiane jest pytanie: „Czy adresy znajdujące się w tym węźle zawierają w kolejnych Z tokenach co najmniej jeden token ze zbioru Y?”.
--	Wszystkie adresy z pozytywną odpowiedzią trafiają do lewego poddrzewa, a z negatywną do prawego.
--	Następne następuje rekurencyjne wywołanie metody dla obu węzłów. Każde kolejne rekurencyjne zagłębienie pomija kolejny jeden token z załadowanych  bloków adresowych (po to aby nie odpytywać jedyne o początek adresu, ale także o jego dalszą cześć).
--	Rekurencyjne wywołania kończą się czy lista adresów w danych węźle jest pusta lub gdy algorytm dojdzie do końca najdłuższego bloku adresowego.
-	W tym momencie utworzone zostało drzewo decyzyjne i proces tworzenia podobnych drzew się powtarza.

Etap działania algorytmu polega na porównaniu załadowanych kolejnych tokenów z wyuczonymi drzewami. Poniżej kroki tego procesu:
-	Załadowanie tokenów z pliku.
-	Iteracja po kolejnych tokenach. Na każdej pozycji stawiane jest pytanie/hipoteza „czy w tym miejscu rozpoczyna się blok adresowy?”. Proces uzyskania odpowiedzi na to pytanie opisują kolejne punkty.
-	Następuje porównanie z każdym drzewem decyzyjnym rozpoczynając od ich korzeni. W każdym węźle  algorytm odpowiada sobie na zapisane w nim pytanie „Czy w kolejnych Z tokenach znajduje się chociaż jeden token ze zbioru Y?”. Jeśli odpowiedź jest twierdząca to algorytm wchodzi do lewego poddrzewa, jeśli przecząca to do prawego. Po dojściu do liścia, algorytm sprawdza czy w tym wyuczonym liściu znajdują się jakieś adresy. Jeśli tak, to drzewo spodziewa się, że w tym miejscu faktycznie może zaczynać się adres. Następuje dodatkowe sprawdzenie czy podczas przechodzenia drzewa liczba twierdzących odpowiedzi stanowi nie mniej niż ok. 40\% odpowiedzi negatywnych. Dzięki temu pomijane są potencjalne pozycje uzyskane z  mylnych drzew.
-	Jeśli większość drzew dała pozytywną odpowiedź, na zadane pytanie „czy w tym miejscu rozpoczyna się blok adresowy?”, to dana pozycja jest dodawana do rezultatu.

\end{document}
